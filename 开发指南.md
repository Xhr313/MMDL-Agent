# MMDL-Agent å¼€å‘æŒ‡å—

æœ¬æ–‡æ¡£è¯¦ç»†è¯´æ˜å¦‚ä½•ç¼–å†™ Toolã€LangChain/LangGraph çš„ä½œç”¨ï¼Œä»¥åŠåç»­ä»£ç è¡¥å……æ–¹å‘ã€‚

---

## ğŸ“š LangChain ä¸ LangGraph çš„ä½œç”¨

### LangGraph çš„ä½œç”¨

**LangGraph** æ˜¯ä¸€ä¸ªç”¨äºæ„å»º**æœ‰çŠ¶æ€ã€å¤šæ­¥éª¤ Agent å·¥ä½œæµ**çš„æ¡†æ¶ã€‚

#### é¡¹ç›®ä¸­çš„ä½¿ç”¨ï¼š

1. **å·¥ä½œæµç¼–æ’**ï¼ˆ`app/core/agent.py`ï¼‰ï¼š
   ```python
   graph = StateGraph(DetectionState)  # å®šä¹‰çŠ¶æ€ç±»å‹
   graph.add_node("load_data", load_data_node)      # æ·»åŠ èŠ‚ç‚¹
   graph.add_node("anomaly_detect", anomaly_detect_node)
   graph.add_node("summarize", summarize_node)
   graph.add_edge("load_data", "anomaly_detect")    # å®šä¹‰èŠ‚ç‚¹è¿æ¥
   graph.add_edge("anomaly_detect", "summarize")
   ```

2. **çŠ¶æ€ç®¡ç†**ï¼ˆ`app/memory/state.py`ï¼‰ï¼š
   - `DetectionState` æ˜¯è´¯ç©¿æ•´ä¸ªå·¥ä½œæµçš„çŠ¶æ€å¯¹è±¡
   - ä½¿ç”¨ `Annotated[..., add]` å®ç°çŠ¶æ€ç´¯åŠ ï¼ˆå¦‚ `logs`ã€`errors`ï¼‰
   - æ¯ä¸ªèŠ‚ç‚¹æ¥æ”¶çŠ¶æ€ï¼Œä¿®æ”¹åè¿”å›æ–°çŠ¶æ€

3. **æ‰§è¡Œæµç¨‹**ï¼ˆ`app/api/main.py`ï¼‰ï¼š
   ```python
   graph = build_graph()
   state = DetectionState(task=task)
   result_state = await graph.ainvoke(state)  # å¼‚æ­¥æ‰§è¡Œæ•´ä¸ªå·¥ä½œæµ
   ```

**LangGraph çš„ä¼˜åŠ¿ï¼š**
- âœ… å¯è§†åŒ–å·¥ä½œæµï¼ˆèŠ‚ç‚¹ + è¾¹ï¼‰
- âœ… çŠ¶æ€è‡ªåŠ¨ä¼ é€’å’Œåˆå¹¶
- âœ… æ”¯æŒæ¡ä»¶åˆ†æ”¯ã€å¾ªç¯ã€ä¸­æ–­
- âœ… å†…ç½®æ£€æŸ¥ç‚¹ï¼ˆcheckpointï¼‰æ”¯æŒï¼Œå¯æ¢å¤æ‰§è¡Œ

---

### LangChain çš„ä½œç”¨ï¼ˆå¾…é›†æˆï¼‰

**LangChain** æ˜¯ä¸€ä¸ªç”¨äº**æ„å»º LLM åº”ç”¨**çš„å·¥å…·é“¾ï¼Œæä¾›ï¼š

1. **LLM è°ƒç”¨æŠ½è±¡**ï¼š
   - ç»Ÿä¸€æ¥å£è°ƒç”¨ OpenAIã€Anthropicã€æœ¬åœ°æ¨¡å‹ç­‰
   - è‡ªåŠ¨å¤„ç† API Keyã€é‡è¯•ã€æµå¼å“åº”

2. **Prompt æ¨¡æ¿ç®¡ç†**ï¼š
   - ç»“æ„åŒ– Prompt ç¼–å†™
   - å˜é‡æ›¿æ¢ã€å¤šè½®å¯¹è¯ç®¡ç†

3. **å·¥å…·é›†æˆ**ï¼š
   - å°†å‡½æ•°å°è£…ä¸º LangChain Tool
   - è‡ªåŠ¨ç”Ÿæˆå·¥å…·æè¿°ä¾› LLM é€‰æ‹©

#### åœ¨ä½ çš„é¡¹ç›®ä¸­éœ€è¦é›†æˆçš„åœ°æ–¹ï¼š

**`app/core/graph.py` çš„ `summarize_node`**ï¼š
```python
# TODO å¯¹ç»“æœè¿›è¡Œæ¶¦è‰²ï¼Œè°ƒç”¨LLMï¼Œæ ¹æ®å¼‚å¸¸ç‚¹çš„æ•°æ®å†™ä¸€æ®µé€šä¿—æ˜“æ‡‚çš„è¯Šæ–­å»ºè®®
async def summarize_node(state: DetectionState) -> DetectionState:
    # æ¨¡æ‹Ÿ LLM æ€»ç»“
    if state.result:
        state.result.summary = "No critical anomalies detected (mock summary)."
    state.logs.append("Summary generated")
    return state
```

**è¿™é‡Œåº”è¯¥ä½¿ç”¨ LangChain è°ƒç”¨ LLM**ï¼Œä¾‹å¦‚ï¼š
```python
from langchain_openai import ChatOpenAI
from langchain.prompts import ChatPromptTemplate

async def summarize_node(state: DetectionState) -> DetectionState:
    if state.result:
        llm = ChatOpenAI(model="gpt-4", temperature=0)
        prompt = ChatPromptTemplate.from_messages([
            ("system", "ä½ æ˜¯ä¸€ä¸ªå·¥ä¸šå¼‚å¸¸æ£€æµ‹ä¸“å®¶..."),
            ("user", "è¯·åˆ†æä»¥ä¸‹å¼‚å¸¸æ•°æ®ï¼š{anomalies}")
        ])
        chain = prompt | llm
        summary = await chain.ainvoke({"anomalies": state.result.anomalies})
        state.result.summary = summary.content
    return state
```

---

## ğŸ› ï¸ Tool ç¼–å†™æŒ‡å—

### Tool æ¶æ„è¯´æ˜

ä½ çš„é¡¹ç›®ä¸­çš„ Tool è®¾è®¡éµå¾ªä»¥ä¸‹æ¨¡å¼ï¼š

```
BaseTool (æŠ½è±¡åŸºç±»)
â”œâ”€â”€ name: str                    # å·¥å…·åç§°
â””â”€â”€ async run(task) -> ToolResponse  # æ‰§è¡Œæ–¹æ³•

ToolResponse (ç»Ÿä¸€å“åº”æ ¼å¼)
â”œâ”€â”€ tool_name: str
â”œâ”€â”€ success: bool
â”œâ”€â”€ result: Optional[DetectionResult]
â””â”€â”€ error: Optional[str]
```

### å¦‚ä½•ç¼–å†™æ–°çš„ Tool

#### 1. ç»§æ‰¿ `BaseTool` å¹¶å®ç° `run` æ–¹æ³•

**ç¤ºä¾‹ï¼šå®ç°ä¸€ä¸ªçœŸå®çš„ HTTP å¼‚å¸¸æ£€æµ‹å·¥å…·**

```python
# app/tools/anomaly_detection.py

import httpx
from typing import Optional
from app.schemas.detection import DetectionTask, DetectionResult, ToolResponse
from app.exceptions.base import ExternalServiceError, ToolExecutionError
from app.config.settings import settings

class HttpAnomalyDetectionTool(BaseTool):
    """é€šè¿‡ HTTP è°ƒç”¨å¤–éƒ¨å¼‚å¸¸æ£€æµ‹æœåŠ¡"""
    
    name = "http_anomaly_detection"
    
    def __init__(self, service_url: Optional[str] = None):
        """
        å‚æ•°:
        - service_url: å¤–éƒ¨æœåŠ¡ URLï¼ˆå¯ä» settings è¯»å–ï¼‰
        """
        self.service_url = service_url or getattr(settings, 'anomaly_detection_url', None)
        if not self.service_url:
            raise ConfigurationError("anomaly_detection_url not configured")
    
    async def run(self, task: DetectionTask) -> ToolResponse:
        """è°ƒç”¨å¤–éƒ¨ HTTP æœåŠ¡æ‰§è¡Œå¼‚å¸¸æ£€æµ‹"""
        try:
            async with httpx.AsyncClient(timeout=30.0) as client:
                # æ„é€ è¯·æ±‚ä½“
                payload = {
                    "task_id": task.task_id,
                    "asset_id": task.asset_id,
                    "start_time": task.start_time,
                    "end_time": task.end_time,
                    "data": task.parameters.get("data", []),  # ä» parameters ä¸­æå–æ•°æ®
                    "threshold": task.parameters.get("threshold", 0.5)
                }
                
                # å‘é€ POST è¯·æ±‚
                response = await client.post(
                    f"{self.service_url}/detect",
                    json=payload,
                    headers={"Content-Type": "application/json"}
                )
                response.raise_for_status()  # æŠ›å‡º HTTP é”™è¯¯
                
                # è§£æå“åº”
                result_data = response.json()
                result = DetectionResult(
                    task_id=result_data["task_id"],
                    status=result_data.get("status", "success"),
                    anomalies=result_data.get("anomalies", []),
                    summary=result_data.get("summary"),
                    metadata=result_data.get("metadata", {})
                )
                
                return ToolResponse(
                    tool_name=self.name,
                    success=True,
                    result=result
                )
                
        except httpx.HTTPStatusError as e:
            raise ExternalServiceError(
                message=f"HTTP service returned {e.response.status_code}",
                details={"url": self.service_url, "status_code": e.response.status_code}
            )
        except httpx.RequestError as e:
            raise ExternalServiceError(
                message=f"Failed to connect to detection service",
                original_error=e
            )
        except Exception as e:
            raise ToolExecutionError(
                tool_name=self.name,
                arguments={"task_id": task.task_id},
                error=e
            )
```

#### 2. åœ¨é…ç½®ä¸­æ·»åŠ æœåŠ¡ URLï¼ˆ`app/config/settings.py`ï¼‰

```python
class Settings(BaseSettings):
    model_config = SettingsConfigDict(env_file=".env", env_prefix="APP_")
    
    app_name: str = "industrial-anomaly-agent"
    log_level: str = "INFO"
    
    # æ–°å¢ï¼šå¼‚å¸¸æ£€æµ‹æœåŠ¡ URL
    anomaly_detection_url: str = "http://localhost:8080/api"
```

#### 3. åœ¨å·¥ä½œæµèŠ‚ç‚¹ä¸­ä½¿ç”¨æ–°å·¥å…·ï¼ˆ`app/core/graph.py`ï¼‰

```python
async def anomaly_detect_node(state: DetectionState) -> DetectionState:
    # æ ¹æ®é…ç½®é€‰æ‹©å·¥å…·ï¼ˆå¯ä»¥ä» settings æˆ– task.parameters ä¸­è¯»å–ï¼‰
    tool_type = state.task.parameters.get("tool_type", "mock")
    
    if tool_type == "http":
        tool = HttpAnomalyDetectionTool()
    else:
        tool = MockAnomalyDetectionTool()
    
    response = await tool.run(state.task)
    if response.success and response.result:
        state.result = response.result
        state.logs.append(f"Anomaly detection completed using {tool.name}")
    else:
        state.errors.append(response.error or "Unknown tool error")
    return state
```

### Tool ç¼–å†™æœ€ä½³å®è·µ

1. **é”™è¯¯å¤„ç†**ï¼š
   - ä½¿ç”¨é¡¹ç›®ç»Ÿä¸€çš„å¼‚å¸¸ç±»å‹ï¼ˆ`ExternalServiceError`ã€`ToolExecutionError`ï¼‰
   - è®°å½•è¯¦ç»†çš„é”™è¯¯ä¸Šä¸‹æ–‡ï¼ˆURLã€çŠ¶æ€ç ã€åŸå§‹å¼‚å¸¸ï¼‰

2. **è¶…æ—¶æ§åˆ¶**ï¼š
   - ä½¿ç”¨ `httpx.AsyncClient(timeout=30.0)` è®¾ç½®è¶…æ—¶
   - é¿å…å·¥å…·æ‰§è¡Œæ—¶é—´è¿‡é•¿å½±å“æ•´ä¸ªå·¥ä½œæµ

3. **æ—¥å¿—è®°å½•**ï¼š
   - åœ¨ `state.logs` ä¸­è®°å½•å·¥å…·æ‰§è¡Œæƒ…å†µ
   - ä½¿ç”¨ `app.utils.logging` è®°å½•è¯¦ç»†æ—¥å¿—

4. **å‚æ•°éªŒè¯**ï¼š
   - åœ¨ `run` æ–¹æ³•å¼€å§‹å¤„éªŒè¯ `task` å‚æ•°
   - ä½¿ç”¨ Pydantic æ¨¡å‹ç¡®ä¿ç±»å‹å®‰å…¨

---

## ğŸš€ åç»­ä»£ç è¡¥å……æ–¹å‘

### 1. é›†æˆ LangChain è°ƒç”¨ LLMï¼ˆä¼˜å…ˆçº§ï¼šé«˜ï¼‰

**ç›®æ ‡**ï¼šåœ¨ `summarize_node` ä¸­ä½¿ç”¨ LangChain è°ƒç”¨ LLM ç”Ÿæˆè¯Šæ–­å»ºè®®

**æ­¥éª¤**ï¼š

1. **å®‰è£… LangChain OpenAI é›†æˆ**ï¼š
   ```bash
   pip install langchain-openai
   ```

2. **åœ¨ `app/config/settings.py` æ·»åŠ  LLM é…ç½®**ï¼š
   ```python
   class Settings(BaseSettings):
       # ... ç°æœ‰é…ç½® ...
       
       # LLM é…ç½®
       openai_api_key: str = ""
       llm_model: str = "gpt-4o-mini"
       llm_temperature: float = 0.3
   ```

3. **åˆ›å»º Prompt æ¨¡æ¿**ï¼ˆ`app/prompts/summarize.py`ï¼‰ï¼š
   ```python
   from langchain.prompts import ChatPromptTemplate
   
   SUMMARIZE_PROMPT = ChatPromptTemplate.from_messages([
       ("system", """ä½ æ˜¯ä¸€ä¸ªå·¥ä¸šå¼‚å¸¸æ£€æµ‹ä¸“å®¶ã€‚è¯·æ ¹æ®æ£€æµ‹åˆ°çš„å¼‚å¸¸æ•°æ®ï¼Œ
       ç”Ÿæˆä¸€æ®µé€šä¿—æ˜“æ‡‚çš„è¯Šæ–­å»ºè®®ï¼ŒåŒ…æ‹¬ï¼š
       1. å¼‚å¸¸ç±»å‹å’Œä¸¥é‡ç¨‹åº¦
       2. å¯èƒ½çš„åŸå› åˆ†æ
       3. å»ºè®®çš„å¤„ç†æªæ–½
       
       ä½¿ç”¨ä¸­æ–‡å›ç­”ï¼Œè¯­è¨€è¦ä¸“ä¸šä½†æ˜“æ‡‚ã€‚"""),
       ("user", """æ£€æµ‹ä»»åŠ¡ ID: {task_id}
       èµ„äº§ ID: {asset_id}
       æ—¶é—´èŒƒå›´: {start_time} è‡³ {end_time}
       
       æ£€æµ‹åˆ°çš„å¼‚å¸¸ï¼š
       {anomalies}
       
       è¯·ç”Ÿæˆè¯Šæ–­å»ºè®®ã€‚""")
   ])
   ```

4. **å®ç° `summarize_node`**ï¼ˆ`app/core/graph.py`ï¼‰ï¼š
   ```python
   from langchain_openai import ChatOpenAI
   from app.prompts.summarize import SUMMARIZE_PROMPT
   from app.config.settings import settings
   from app.exceptions.base import ModelError
   
   async def summarize_node(state: DetectionState) -> DetectionState:
       """ä½¿ç”¨ LLM ç”Ÿæˆè¯Šæ–­å»ºè®®"""
       if not state.result:
           state.logs.append("No result to summarize")
           return state
       
       try:
           # åˆå§‹åŒ– LLM
           llm = ChatOpenAI(
               model=settings.llm_model,
               temperature=settings.llm_temperature,
               api_key=settings.openai_api_key
           )
           
           # æ„é€  Prompt
           prompt = SUMMARIZE_PROMPT.format_messages(
               task_id=state.task.task_id,
               asset_id=state.task.asset_id,
               start_time=state.task.start_time,
               end_time=state.task.end_time,
               anomalies=state.result.anomalies
           )
           
           # è°ƒç”¨ LLM
           response = await llm.ainvoke(prompt)
           
           # æ›´æ–°ç»“æœ
           state.result.summary = response.content
           state.logs.append("LLM summary generated")
           
       except Exception as e:
           raise ModelError(
               message="Failed to generate summary",
               model_name=settings.llm_model,
               original_error=e
           )
       
       return state
   ```

---

### 2. å®ç°æ£€æŸ¥ç‚¹æŒä¹…åŒ–ï¼ˆä¼˜å…ˆçº§ï¼šä¸­ï¼‰

**ç›®æ ‡**ï¼šå®ç° `app/memory/checkpoint.py` çš„ `CheckpointStore`ï¼Œæ”¯æŒçŠ¶æ€æŒä¹…åŒ–

**æ­¥éª¤**ï¼š

1. **å®ç°å†…å­˜æ£€æŸ¥ç‚¹**ï¼ˆç”¨äºå¼€å‘æµ‹è¯•ï¼‰ï¼š
   ```python
   # app/memory/checkpoint.py
   
   from typing import Dict, Optional
   from app.memory.state import DetectionState
   
   class MemoryCheckpointStore(CheckpointStore):
       """å†…å­˜æ£€æŸ¥ç‚¹å­˜å‚¨ï¼ˆä»…ç”¨äºå¼€å‘ï¼‰"""
       
       def __init__(self):
           self._store: Dict[str, DetectionState] = {}
       
       def save(self, state: DetectionState):
           """ä¿å­˜çŠ¶æ€"""
           self._store[state.task.task_id] = state
       
       def load(self, task_id: str) -> Optional[DetectionState]:
           """åŠ è½½çŠ¶æ€"""
           return self._store.get(task_id)
   ```

2. **å®ç° Redis æ£€æŸ¥ç‚¹**ï¼ˆç”¨äºç”Ÿäº§ï¼‰ï¼š
   ```python
   import json
   import redis.asyncio as redis
   from app.memory.state import DetectionState
   
   class RedisCheckpointStore(CheckpointStore):
       """Redis æ£€æŸ¥ç‚¹å­˜å‚¨"""
       
       def __init__(self, redis_url: str = "redis://localhost:6379"):
           self.redis_client = redis.from_url(redis_url)
       
       async def save(self, state: DetectionState):
           """ä¿å­˜çŠ¶æ€åˆ° Redis"""
           key = f"checkpoint:{state.task.task_id}"
           value = state.model_dump_json()
           await self.redis_client.setex(key, 3600, value)  # 1å°æ—¶è¿‡æœŸ
       
       async def load(self, task_id: str) -> Optional[DetectionState]:
           """ä» Redis åŠ è½½çŠ¶æ€"""
           key = f"checkpoint:{task_id}"
           value = await self.redis_client.get(key)
           if value:
               return DetectionState.model_validate_json(value)
           return None
   ```

3. **åœ¨ LangGraph ä¸­å¯ç”¨æ£€æŸ¥ç‚¹**ï¼ˆ`app/core/agent.py`ï¼‰ï¼š
   ```python
   from langgraph.checkpoint.memory import MemorySaver
   from app.memory.checkpoint import MemoryCheckpointStore
   
   def build_graph(checkpoint_store=None):
       """æ„å»ºå¹¶ç¼–è¯‘æ£€æµ‹å·¥ä½œæµå›¾"""
       graph = StateGraph(DetectionState)
       # ... æ·»åŠ èŠ‚ç‚¹å’Œè¾¹ ...
       
       # å¯ç”¨æ£€æŸ¥ç‚¹
       if checkpoint_store is None:
           checkpoint_store = MemorySaver()  # LangGraph å†…ç½®å†…å­˜å­˜å‚¨
       
       return graph.compile(checkpointer=checkpoint_store)
   ```

---

### 3. æ·»åŠ æ›´å¤šå·¥å…·ç±»å‹ï¼ˆä¼˜å…ˆçº§ï¼šä¸­ï¼‰

**ç¤ºä¾‹ï¼šæ•°æ®åº“æŸ¥è¯¢å·¥å…·**

```python
# app/tools/database_query.py

from app.tools.anomaly_detection import BaseTool
from app.schemas.detection import DetectionTask, ToolResponse
import asyncpg  # æˆ–å…¶ä»–å¼‚æ­¥æ•°æ®åº“é©±åŠ¨

class DatabaseQueryTool(BaseTool):
    """ä»æ•°æ®åº“æŸ¥è¯¢å†å²æ•°æ®"""
    
    name = "database_query"
    
    async def run(self, task: DetectionTask) -> ToolResponse:
        # è¿æ¥æ•°æ®åº“ï¼ŒæŸ¥è¯¢å†å²æ•°æ®
        # è¿”å›æŸ¥è¯¢ç»“æœ
        pass
```

**ç¤ºä¾‹ï¼šæ–‡ä»¶ç³»ç»Ÿå·¥å…·**

```python
# app/tools/file_operations.py

class FileReadTool(BaseTool):
    """è¯»å–æ–‡ä»¶å†…å®¹"""
    
    name = "file_read"
    
    async def run(self, task: DetectionTask) -> ToolResponse:
        file_path = task.parameters.get("file_path")
        # è¯»å–æ–‡ä»¶å¹¶è¿”å›å†…å®¹
        pass
```

---

### 4. å¢å¼ºå·¥ä½œæµï¼šæ·»åŠ æ¡ä»¶åˆ†æ”¯å’Œå¾ªç¯ï¼ˆä¼˜å…ˆçº§ï¼šä½ï¼‰

**ç¤ºä¾‹ï¼šæ ¹æ®æ£€æµ‹ç»“æœå†³å®šæ˜¯å¦ç»§ç»­å¤„ç†**

```python
# app/core/graph.py

def should_continue(state: DetectionState) -> str:
    """æ¡ä»¶å‡½æ•°ï¼šæ ¹æ®ç»“æœå†³å®šä¸‹ä¸€æ­¥"""
    if state.result and len(state.result.anomalies) > 0:
        return "detailed_analysis"  # æœ‰å¼‚å¸¸ï¼Œè¿›å…¥è¯¦ç»†åˆ†æ
    return "end"  # æ— å¼‚å¸¸ï¼Œç»“æŸ

# åœ¨ agent.py ä¸­ä½¿ç”¨
graph.add_conditional_edges(
    "anomaly_detect",
    should_continue,
    {
        "detailed_analysis": "summarize",
        "end": END
    }
)
```

---

### 5. æ·»åŠ æ•°æ®éªŒè¯å’Œé¢„å¤„ç†èŠ‚ç‚¹ï¼ˆä¼˜å…ˆçº§ï¼šä¸­ï¼‰

**åœ¨ `load_data_node` ä¸­æ·»åŠ æ•°æ®éªŒè¯**ï¼š

```python
async def load_data_node(state: DetectionState) -> DetectionState:
    """åŠ è½½å¹¶éªŒè¯æ•°æ®"""
    if not state.task:
        raise DataMissingError("DetectionTask missing")
    
    # éªŒè¯æ—¶é—´èŒƒå›´
    from datetime import datetime
    start = datetime.fromisoformat(state.task.start_time.replace('Z', '+00:00'))
    end = datetime.fromisoformat(state.task.end_time.replace('Z', '+00:00'))
    if start >= end:
        raise DataMissingError("start_time must be before end_time")
    
    # éªŒè¯æ•°æ®
    data = state.task.parameters.get("data", [])
    if not data or len(data) == 0:
        raise DataMissingError("data parameter is required and cannot be empty")
    
    state.context["loaded"] = True
    state.context["data_points"] = len(data)
    state.logs.append(f"Data loaded: {len(data)} points")
    return state
```

---

## ğŸ“ ä»£ç è¡¥å……æ£€æŸ¥æ¸…å•

- [ ] **é›†æˆ LangChain LLM**ï¼š
  - [ ] å®‰è£… `langchain-openai`
  - [ ] åœ¨ `settings.py` æ·»åŠ  LLM é…ç½®
  - [ ] åˆ›å»º Prompt æ¨¡æ¿ï¼ˆ`app/prompts/summarize.py`ï¼‰
  - [ ] å®ç° `summarize_node` è°ƒç”¨ LLM
  - [ ] æ·»åŠ é”™è¯¯å¤„ç†å’Œé‡è¯•é€»è¾‘

- [ ] **å®ç°æ£€æŸ¥ç‚¹æŒä¹…åŒ–**ï¼š
  - [ ] å®ç° `MemoryCheckpointStore`
  - [ ] ï¼ˆå¯é€‰ï¼‰å®ç° `RedisCheckpointStore`
  - [ ] åœ¨ `build_graph` ä¸­å¯ç”¨æ£€æŸ¥ç‚¹
  - [ ] æ·»åŠ çŠ¶æ€æ¢å¤ API ç«¯ç‚¹

- [ ] **å®Œå–„ HTTP å·¥å…·**ï¼š
  - [ ] å®ç° `HttpAnomalyDetectionTool.run()`
  - [ ] æ·»åŠ è¶…æ—¶å’Œé‡è¯•é€»è¾‘
  - [ ] æ·»åŠ è¯·æ±‚/å“åº”æ—¥å¿—
  - [ ] ç¼–å†™å•å…ƒæµ‹è¯•

- [ ] **å¢å¼ºå·¥ä½œæµ**ï¼š
  - [ ] æ·»åŠ æ•°æ®éªŒè¯èŠ‚ç‚¹
  - [ ] æ·»åŠ æ¡ä»¶åˆ†æ”¯é€»è¾‘
  - [ ] æ·»åŠ é”™è¯¯æ¢å¤æœºåˆ¶
  - [ ] å®ç° `current_step` çš„å¾ªç¯æ£€æµ‹

- [ ] **æ·»åŠ æ›´å¤šå·¥å…·**ï¼š
  - [ ] æ•°æ®åº“æŸ¥è¯¢å·¥å…·
  - [ ] æ–‡ä»¶æ“ä½œå·¥å…·
  - [ ] å¤–éƒ¨ API è°ƒç”¨å·¥å…·

---

## ğŸ”— å‚è€ƒèµ„æº

- [LangGraph å®˜æ–¹æ–‡æ¡£](https://langchain-ai.github.io/langgraph/)
- [LangChain å®˜æ–¹æ–‡æ¡£](https://python.langchain.com/)
- [LangChain Tool é›†æˆæŒ‡å—](https://python.langchain.com/docs/modules/tools/)
- [FastAPI å¼‚æ­¥æœ€ä½³å®è·µ](https://fastapi.tiangolo.com/async/)

---

